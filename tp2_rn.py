# -*- coding: utf-8 -*-
"""TP2_RN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4vbUHZXSRNPMMoroS6gIyGarKeci621
"""

import pandas as pd
import os
import tensorflow as tf
import numpy as np
#1- 1- Créer Un DataFrame en utilisant les données de fichier ‘’Iris.csv” qui contient notre dataSet.
iris_data = pd.read_csv("Iris.csv");

#2-Afficher les 10 premières lignes du DataFrame
iris_data.head(10)

#3-Afficher les dimensions du dataframe
iris_data.shape

#4-Utiliser la bibliothèque python Seaborn pour visualiser les données en fonction de la longueur des pétales et de largeur des sépales.
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn
seaborn.set_style('whitegrid')
df = seaborn.load_dataset("iris")
seaborn.pairplot(df, hue="species");

#5-Ecrire un script python permettant de labelliser les différentes espèces d’iris.( Iris_Setosa → 0 , Iris_Versicolor→ 1 et Iris_Virginica → 2 )
iris_data.loc[iris_data["Species"] == "Iris-setosa" , "Species"] = 0
iris_data.loc[iris_data["Species"] == "Iris-versicolor" , "Species"] = 1
iris_data.loc[iris_data["Species"] == "Iris-virginica" , "Species"] = 2

#6-Afficher du nouveau les 10 premières lignes du nouveau DataFrame contenant les labels.
iris_data.head(10)

#7-Écrire un script python permettant la division de dataset en des données d’apprentissage (70%) et des données de test (30%).

from sklearn.model_selection import train_test_split
X_data = iris_data.iloc[:, 1:5].values
Y_data = iris_data.iloc[:, 5].values
X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.3)

#8-Afficher les 10 premières données d’apprentissage et celles de test.
X_train[:10]

#9-Ecrire un script python qui utilise un perceptron multicouche pour l’apprentissage des données avec un optimisateur (‘lbfgs’, epsilon=0.07 et nombre maximum d’itération=150) .
from sklearn.neural_network import MLPClassifier
classifier = MLPClassifier(solver="lbfgs",epsilon=0.07,max_iter=150).fit(X_train.astype('int'), Y_train.astype('int'))
predModel=classifier.predict(X_test.astype('int'))
predModel

#10-Evaluer ce perceptron en affichant son “accuracy” et le temps de réponse.
from sklearn import metrics
print(metrics.accuracy_score(Y_test.astype('int'),predModel))

#11-Afficher la matrice de confusion associée à notre cas.
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
metrics.confusion_matrix(Y_test.astype('int'),predModel)

"""#12-Selon les résultats affichés commenter le perceptron que vous avez utilisé.
L'axe des x présente les valeurs prédite et des Y présente les valeurs réelles. Les valeurs (pour les X et les Y ) sont ordonnées respectivement:Setosa ,Versicolor et Virginica. D'après la matrice de confusion => 14 valeurs classées comme Setosa sont en réalité Virginica, et 13 comme Virginica mais elles sont Setosa et les 18 autres sont bien classées .
"""

#13-On va ajouter le paramètre de taux d’apprentissage au niveau de classifieur utilisé pour une valeur égale 0.7
params = [
    {
        "solver": "sgd",
        "learning_rate": "constant",
        "learning_rate_init": 0.2,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "constant",
        "learning_rate_init": 0.7,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "invscaling",
        "learning_rate_init": 0.2,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "invscaling",
        "learning_rate_init": 0.7,
        "max_iter" : 150
    },
    {"solver": "adam", "learning_rate_init": 0.01},
]

labels = [
    "constant learning-rate 0.2",
    "constant learning-rate 0.7",
    "inv-scaling learning-rate 0.2",
    "inv-scaling learning-rate 0.7",
    "adam 0.01",
]

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

scaler = sc.fit(X_train)
trainX_scaled = scaler.transform(X_train)
testX_scaled = scaler.transform(X_test)

dataClassifiers = []
for i in range(len(params)):
  classifier = MLPClassifier(random_state=0, **params[i])
  classifier.fit(X_train, Y_train.astype('int'))
  print(labels[i]," : ",classifier.score(X_train, Y_train.astype('int')))
  dataClassifiers.append(classifier)

#14-Nous allons étudier la variation du paramètre de taux d’apprentissage,donc nous allons afficher la courbe d'évolution d’apprentissage et celle de test en fonction de variation du taux d’apprentissage.
plot_args = [
    {"c": "red", "linestyle": "-"},
    {"c": "green", "linestyle": "-"},
    {"c": "blue", "linestyle": "-"},
    {"c": "red", "linestyle": "--"},
    {"c": "green", "linestyle": "--"},
]

for i in range(len(dataClassifiers)):
  plt.plot(dataClassifiers[i].loss_curve_, **plot_args[i])
  plt.title(labels[i], fontsize=14)
  plt.show()

#15-Nous allons fixer un nombre d’itération égale à 10 fois le nombre fixé au début. Quel est le phénomène constaté ? et comment pouvons- nous surmonter celui-ci ?
TrainModel=MLPClassifier(solver="lbfgs",epsilon=0.07,max_iter=1500).fit(X_train.astype('int'), Y_train.astype('int'))
TrainpredModel=TrainModel.predict(X_train.astype('int'))
metrics.accuracy_score(Y_train.astype('int'),TrainpredModel)

#16-Tester d’autres classifieurs de type réseau de neurones (Dynamique/Récurrent et Non récurrent) .
from keras.models import Sequential
from keras.layers import Dense,LSTM,SimpleRNN
Seqmodel = Sequential()
Seqmodel.add(Dense(len(X_train),input_shape=(len(X_train),),activation='relu'))
Seqmodel.add(Dense(1,activation='softmax'))
Seqmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
model = Seqmodel.fit(X_train.astype('int'), Y_train.astype('int'))

#17-Dans notre cas de base de donnée Iris, est ce qu’il est intéressant d’avoir un réseau de neurones récurrent ou non ? justifier vos réponses.
Seqmodel = SimpleRNN(len(X_train),return_sequences=True, return_state=True)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
history = model.fit(X_train.astype('float'), Y_train.astype('float'))